---
title: "Homework #2: Resampling" 
author: "Richard 'Ricky' Kuehn"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
data_dir = 'https://mdporter.github.io/teaching/data/' # data directory
library(tidymodels)# for optional tidymodels solutions
library(tidyverse) # functions for data manipulation
library(caret)
```


# Problem 1: Bootstrapping 

Bootstrap resampling can be used to quantify the uncertainty in a fitted curve. 

## a. Data Generating Process

Create a set of functions to generate data from the following distributions:
\begin{align*}
X &\sim \mathcal{U}(0, 2) \qquad \text{Uniform between $0$ and $2$}\\
Y &= 1 + 2x + 5\sin(5x) + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma=2.5)
\end{align*}

::: {.callout-note title="Solution"}
```{r}
# generate x
sim_x <- function(n) runif(n, 0, 2) 

# true mean function
f <- function(x) 1 + 2*x + 5*sin(5*x) 

# generate y
sim_y <- function(x) { 
    n = length(x)
    f(x) + rnorm(n, sd=2.5)
}
```
:::

## b. Simulate data

Simulate $n=100$ realizations from these distributions. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$. Use `set.seed(211)` prior to generating the data.

::: {.callout-note title="Solution"}
```{r}
# n
n = 100

# set seed, generate data
set.seed(211)
x = sim_x(n)
y = sim_y(x)
data_i = tibble(x,y)

# plot
ggplot(data_i, aes(x, y)) +
    geom_point() +
    geom_function(fun=f, color='blue') +
    labs(title="scatterplot with true regression", color='')
```
:::


## c. 5th degree polynomial fit

Fit a 5th degree polynomial. Produce a scatterplot and draw the *estimated* regression curve.

::: {.callout-note title="Solution"}
```{r}
# fit 5th degree polynomial
fit_5th = lm(y~poly(x,5))

# estimates
xseq = seq(0, 2, length=100)
yhat = predict(fit_5th, tibble(x=xseq))

# plot
ggplot(data_i, aes(x, y)) +
    geom_point() +
    geom_function(fun=f, aes(color='true regression'), linetype='dotted') +
    geom_line(aes(xseq, yhat, color='5th degree polynomial')) +
    labs(title="scatterplot with true, 5th degree polynomial regressions", color='')
```
:::


## d. Bootstrap sampling

Make 200 bootstrap samples. For each bootstrap sample, fit a 5th degree polynomial and make predictions at `eval_pts = seq(0, 2, length=100)`

- Set the seed (use `set.seed(212)`) so your results are reproducible.
- Produce a scatterplot with the original data and add the 200 bootstrap curves

::: {.callout-note title="Solution"}
```{r}
# Set seed
set.seed(212)

# Set M, eval_pts
M <- 200  # 200 bootstraps
eval_pts <- seq(0, 2, length = 100)  # evaluation sequence

# Bootstrap loop
bootstraps <- map_dfc(1:M, ~ {  #purr
  data_boot <- data_i |> slice_sample(n = nrow(data_i), replace = TRUE)
  m_boot <- lm(y ~ poly(x, 5), data = data_boot)
  predict(m_boot, newdata = tibble(x = eval_pts))
}) |>
  set_names(paste0("bootstrap_", 1:M))

# Add eval points and reshape
bootstraps <- bootstraps |>
  bind_cols(x = eval_pts) |>
  pivot_longer(-x, names_to = 'bootstrap', values_to = 'yhat')

# Plot
ggplot() +
  geom_point(data = data_i, aes(x = x, y = y)) +
  geom_line(data = bootstraps, aes(x = x, y = yhat, group = bootstrap), 
            color = "red", alpha = 0.1) +
  labs(title = "scatterplot with bootstrap curves",
       x = "x", y = "y")
```
:::
    
## e. Confidence Intervals

Calculate the pointwise 95% confidence intervals from the bootstrap samples. That is, for each $x \in {\rm eval\_pts}$, calculate the upper and lower limits such that only 5% of the curves fall outside the interval at $x$. 

- Remake the plot from part *c*, but add the upper and lower boundaries from the 95% confidence intervals. 

::: {.callout-note title="Solution"}
```{r}
# group by x values and find confidence interval
ci_data <- bootstraps |>
  group_by(x) |>
  summarise(
    lower = quantile(yhat, 0.025),
    upper = quantile(yhat, 0.975),
    .groups = 'drop'
  )

ci_data

# plot graph c with CI
ggplot(data_i, aes(x, y)) +
    geom_point() +
    geom_function(fun=f, aes(color='true regression')) +
    geom_line(aes(xseq, yhat, color='5th degree polynomial')) +
    geom_ribbon(data = ci_data, aes(x = x, ymin = lower, ymax = upper),
              fill = "red", alpha = 0.2) +
    geom_line(data = ci_data, aes(x = x, y = lower), color = "red", linetype = "dashed") +
    geom_line(data = ci_data, aes(x = x, y = upper), color = "red", linetype = "dashed") +
    labs(title = "scatterplot with 95% CI",
       x = "x", y = "y")
```
Add solution here

:::

# Problem 2: V-Fold cross-validation with $k$ nearest neighbors

Run 10-fold cross-validation on the data generated in part 1b to select the optimal $k$ in a k-nearest neighbor (kNN) model. Then evaluate how well cross-validation performed by evaluating the performance on a large test set. The steps below will guide you.


## a. Implement 10-fold cross-validation

Use $10$-fold cross-validation to find the value of $k$ (i.e., neighborhood size) that provides the smallest cross-validated MSE using a kNN model. 

- Search over $k=3,4,\ldots, 40$.
- Use `set.seed(221)` prior to generating the folds to ensure the results are replicable. 
- Show the following:
    - the optimal $k$ (as determined by cross-validation)
    - the corresponding estimated MSE
    - produce a plot with $k$ on the x-axis and the estimated MSE on the y-axis (optional: add 1-standard error bars). 
- Notation: The $k$ is the tuning paramter for the kNN model. The $v=10$ is the number of folds in V-fold cross-validation. Don't get yourself confused.

::: {.callout-note title="Solution"}
```{r}
# set seed
set.seed(221)

# training control
train_ctrl <- trainControl(method = "repeatedcv", number = 10)

# model with 10-fold CV and tune for k from 3 to 40
knn_model <- train(y ~ x, data = data_i, method = "knn", trControl = train_ctrl,
                   tuneGrid = expand.grid(k = 3:40))

results_a <- knn_model$results |>
  mutate(MSE = RMSE^2)

# lowest mse
best_a <- results_a |>
  slice_min(MSE) |>
  select(k, MSE)

# data for plotting
plot_data <- tibble(
  k = results_a$k,
  MSE = results_a$MSE
)

# plot
ggplot(plot_data, aes(x = k, y = MSE)) +
  geom_line() +
  geom_point() +
  geom_point(data = results_a %>% filter(k == best_a$k), aes(x = k, y = MSE), color = "red", size = 3) +
  labs(title = "kNN model performance (MSE)", x = "k (neighbors)", y = "MSE")
```
```{r}
best_a
```
:::


## b. Find the optimal *edf*

The $k$ (number of neighbors) in a kNN model determines the effective degrees of freedom *edf*. What is the optimal *edf*? Be sure to use the correct sample size when making this calculation. Produce a plot similar to that from part *a*, but use *edf* (effective degrees of freedom) on the x-axis. 

::: {.callout-note title="Solution"}
```{r}
# calculate edf
n <- nrow(data_i)
results_b <- results_a %>%
  mutate(edf = n / k) # edf formula

# find edf at lowest mse
best_b <- results_b |>
  slice_min(MSE) |>
  select(k, MSE, edf)

# data for plotting edf and mse
plot_data <- tibble(
  edf = results_b$edf,
  MSE = results_b$MSE
)

# plot
ggplot(plot_data, aes(x = edf, y = MSE)) +
  geom_line() +
  geom_point() +
  geom_point(data = results_b %>% filter(k == best_b$k), aes(x = edf, y = MSE), color = "red", size = 3) + 
  labs(title = "kNN Model Performance (MSE vs EDF)", x = "Effective Degrees of Freedom (EDF)", y = "MSE") +
  theme_minimal()
```
```{r}
best_b
```
:::

## c. Choose $k$

After running cross-validation, a final model fit from *all* of the training data needs to be produced to make predictions. What value of $k$ would you choose? Why? 

::: {.callout-note title="Solution"}
We would want to use kNN = 11 because it has the lowest MSE and has the best balance between bias and variance.
If we went lower, it would lead to overfitting while higher k-values would lead to underfitting.
:::

## d. Evaluate actual performance

Now we will see how well cross-validation performed. Simulate a test data set of $50000$ observations from the same distributions. Use `set.seed(223)` prior to generating the test data. 

- Fit a set of kNN models, using the full training data, and calculate the mean squared error (MSE) on the test data for each model. Use the same $k$ values in *a*. 
- Report the optimal $k$, the corresponding *edf*, and MSE based on the test set.

::: {.callout-note title="Solution"}
:::
```{r}
# set seed
set.seed(223)

# generate test data
n_test = 50000
x_test = sim_x(n_test)
y_test = sim_y(x_test)
data_test = tibble(x = x_test, y = y_test)

# k-range, store mse
k_values <- 3:40
test_mse <- numeric(length(k_values))

# loop through k values
for (i in seq_along(k_values)) {
  k <- k_values[i]
  knn_fit <- train(y ~ x, data = data_i, method = "knn",
                   tuneGrid = expand.grid(k = k),
                   trControl = trainControl(method = "none"))
  y_pred <- predict(knn_fit, newdata = data_test)
  test_mse[i] <- mean((y_test - y_pred)^2)
}

# combine k values and mse
results_c <- tibble(k = k_values, mse = test_mse)

# add edf column
n <- nrow(data_i)
results_c <- results_c %>%
  mutate(edf = n / k) # edf formula

# find minimum mse
best_c <- results_c |>
  slice_min(mse) |>
  select(k, mse, edf)

best_c
```


## e. Performance plots

Plot both the cross-validation estimated and (true) error calculated from the test data on the same plot. See Figure 5.6 in ISL (pg 182) as a guide. 

- Produce two plots: one with $k$ on the x-axis and one with *edf* on the x-axis.
- Each plot should have two lines: one from part *a* and one from part *d* 
    
::: {.callout-note title="Solution"}
```{r}
# combine cv and test results
combined_results <- results_a %>%
  rename(cv_mse = MSE) %>%
  inner_join(results_c %>% select(k, mse), by = "k") %>%
  rename(test_mse = mse)

# Plot with k on the x-axis
ggplot(combined_results, aes(x = k)) +
  geom_line(aes(y = cv_mse, color = "CV MSE")) +
  geom_line(aes(y = test_mse, color = "Test MSE")) +
  geom_point(aes(y = cv_mse, color = "CV MSE")) +
  geom_point(aes(y = test_mse, color = "Test MSE")) +
  labs(title = "kNN Model Performance (MSE)", x = "k (neighbors)", y = "MSE") +
  scale_color_manual(name = "Error", values = c("CV MSE" = "blue", "Test MSE" = "red")) +
  theme_minimal()
```
```{r}
# Plot with edf on the x-axis
ggplot(combined_results, aes(x = n / k)) +
  geom_line(aes(y = cv_mse, color = "CV MSE")) +
  geom_line(aes(y = test_mse, color = "Test MSE")) +
  geom_point(aes(y = cv_mse, color = "CV MSE")) +
  geom_point(aes(y = test_mse, color = "Test MSE")) +
  labs(title = "kNN Model Performance (MSE vs EDF)", x = "Effective Degrees of Freedom (EDF)", y = "MSE") +
  scale_color_manual(name = "Error", values = c("CV MSE" = "blue", "Test MSE" = "red")) +
  theme_minimal()
```

:::
    
## f. Did cross-validation work as intended?

Based on the plots from *e*, does it appear that cross-validation worked as intended? How sensitive is the choice of $k$ on the resulting test MSE?      

::: {.callout-note title="Solution"}
It's honestly not terrible! The shapes are generally the same, especially within a couple k-values of the one we identified
as the best for the model. It certainly appears that the lowest mse from our k-fold cross-validation is a good choice for the test set.
:::




